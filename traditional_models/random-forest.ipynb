{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model\n",
    "All datasets below will be passed through a `Stratified K-Fold` and a `Random Forest Model` to test for accuracy.\n",
    "The best performing training dataset will be used as the main dataset for tuning the Random Forest Model.\n",
    "#### Why are we using Random Forest Model for this project?\n",
    "- `Handling high-dimensional data:`\n",
    "    - Random Forest can efficiently handle high-dimensional data, which is suitable for text embeddings that often result in a large number of features (dimensions).\n",
    "- `Robustness to noise:` \n",
    "    - Tweets may contain noise in the form of misspellings, slang, or abbreviations. Random Forest is robust to noise due to its ensemble nature, aggregating the output of multiple decision trees to make a final prediction.\n",
    "- `Model interpretability:` \n",
    "    - Although not as interpretable as simple decision trees, Random Forest still provides some level of interpretability by calculating feature importances. This can help in understanding which dimensions of the embeddings are most important for the - classification task.\n",
    "- `Reduced overfitting:`\n",
    "    - Random Forest reduces the risk of overfitting by constructing multiple decision trees and combining their outputs. This ensemble approach leads to a more generalized model compared to a single decision tree.\n",
    "- `Built-in feature selection:`\n",
    "    - Random Forest inherently performs feature selection by considering a random subset of features for each tree. This can be particularly beneficial when working with high-dimensional data, such as text embeddings.\n",
    "- `Ease of implementation: `\n",
    "    - Random Forest is easy to implement using popular machine learning libraries like scikit-learn. It also requires minimal preprocessing and hyperparameter tuning, making it a good choice for a fast initial implementation.\n",
    "- `Parallelism:`\n",
    "    - The nature of Random Forest allows for parallel processing, which can significantly speed up training and prediction times, especially when dealing with large datasets.\n",
    "- `Performance:`\n",
    "    - Random Forest has been known to perform well in various classification tasks, making it a reliable choice for your binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_glove_0v = pd.read_csv('../numerical_datasets/train_data_mod_glove_50d_0v_numerical.csv')\n",
    "test_glove_0v =  pd.read_csv('../numerical_datasets/test_data_mod_glove_50d_0v_numerical.csv')\n",
    "train_glove_custom = pd.read_csv('../numerical_datasets/train_data_mod_glove_50d_custom_numerical.csv')\n",
    "test_glove_custom =  pd.read_csv('../numerical_datasets/test_data_mod_glove_50d_custom_numerical.csv')\n",
    "train_fasttext = pd.read_csv('../numerical_datasets/train_data_mod_fasttext_300d_numerical.csv')\n",
    "test_fasttext =  pd.read_csv('../numerical_datasets/test_data_mod_fasttext_300d_numerical.csv')\n",
    "train_word2vec = pd.read_csv('../numerical_datasets/train_data_mod_word2vec_50d_numerical.csv')\n",
    "test_word2vec =  pd.read_csv('../numerical_datasets/test_data_mod_word2vec_50d_numerical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'keyword_encoded', 'tweet_length', 'punctuation_count',\n",
       "       'embedding_0', 'embedding_1', 'embedding_2', 'embedding_3',\n",
       "       'embedding_4', 'embedding_5', 'embedding_6', 'embedding_7',\n",
       "       'embedding_8', 'embedding_9', 'embedding_10', 'embedding_11',\n",
       "       'embedding_12', 'embedding_13', 'embedding_14', 'embedding_15',\n",
       "       'embedding_16', 'embedding_17', 'embedding_18', 'embedding_19',\n",
       "       'embedding_20', 'embedding_21', 'embedding_22', 'embedding_23',\n",
       "       'embedding_24', 'embedding_25', 'embedding_26', 'embedding_27',\n",
       "       'embedding_28', 'embedding_29', 'embedding_30', 'embedding_31',\n",
       "       'embedding_32', 'embedding_33', 'embedding_34', 'embedding_35',\n",
       "       'embedding_36', 'embedding_37', 'embedding_38', 'embedding_39',\n",
       "       'embedding_40', 'embedding_41', 'embedding_42', 'embedding_43',\n",
       "       'embedding_44', 'embedding_45', 'embedding_46', 'embedding_47',\n",
       "       'embedding_48', 'embedding_49'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_glove_0v.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['keyword_encoded', 'tweet_length', 'punctuation_count',\n",
    "       'embedding_0', 'embedding_1', 'embedding_2', 'embedding_3',\n",
    "       'embedding_4', 'embedding_5', 'embedding_6', 'embedding_7',\n",
    "       'embedding_8', 'embedding_9', 'embedding_10', 'embedding_11',\n",
    "       'embedding_12', 'embedding_13', 'embedding_14', 'embedding_15',\n",
    "       'embedding_16', 'embedding_17', 'embedding_18', 'embedding_19',\n",
    "       'embedding_20', 'embedding_21', 'embedding_22', 'embedding_23',\n",
    "       'embedding_24', 'embedding_25', 'embedding_26', 'embedding_27',\n",
    "       'embedding_28', 'embedding_29', 'embedding_30', 'embedding_31',\n",
    "       'embedding_32', 'embedding_33', 'embedding_34', 'embedding_35',\n",
    "       'embedding_36', 'embedding_37', 'embedding_38', 'embedding_39',\n",
    "       'embedding_40', 'embedding_41', 'embedding_42', 'embedding_43',\n",
    "       'embedding_44', 'embedding_45', 'embedding_46', 'embedding_47',\n",
    "       'embedding_48', 'embedding_49']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for K-fold cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Instantiate the Random Forest classifier\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Instantiate the Stratified K-fold cross-validator\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the function that will be used to train and evaluate the model's accuracy based on the Stratified K-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(rfc, X, y):\n",
    "    # Initialize lists to store accuracy and F1 scores for each fold\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Loop through each fold of the K-fold cross-validator\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Split the data into training and validation sets for this fold\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the Random Forest classifier on the training data\n",
    "        rfc.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the validation data\n",
    "        y_pred = rfc.predict(X_val)\n",
    "\n",
    "        # Calculate the validation accuracy score for this fold\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "        # Calculate the validation F1 score for this fold\n",
    "        f1 = f1_score(y_val, y_pred, average='binary')  # Use 'weighted' for multi-class problems\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the validation accuracy and F1 scores\n",
    "    mean_accuracy = np.mean(accuracy_scores)\n",
    "    std_accuracy = np.std(accuracy_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "\n",
    "    # Print the mean and standard deviation of the validation accuracy and F1 scores\n",
    "    print(f\"Mean Accuracy: {mean_accuracy:.4f} +/- {std_accuracy:.4f}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.4f} +/- {std_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_scorer(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    return acc * f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's discuss the hyperparameters in the RFC model\n",
    "\n",
    "`criterion`: This hyperparameter determines the function used to measure the quality of a split when constructing the decision trees within the random forest. The two supported criteria in scikit-learn's RandomForestClassifier are:\n",
    "\n",
    "`gini`: Gini impurity, which measures how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Lower Gini impurity indicates a better split.\n",
    "\n",
    "`entropy`: Information gain, which is based on the concept of entropy from information theory. A split with higher information gain results in a more homogeneous child node, making it a better split.\n",
    "There is no universally best criterion, as the choice of the criterion may depend on the specific problem and dataset. By including both options in the parameter space, the Bayesian optimization will determine the best criterion for your specific problem.\n",
    "\n",
    "`max_features`: This hyperparameter controls the number of features to consider when looking for the best split in each decision tree within the random forest. Considering a subset of features introduces randomness and diversity in the trees, which can lead to a more generalized model. The options for this hyperparameter in scikit-learn's RandomForestClassifier are:\n",
    "\n",
    "`auto`: Equivalent to sqrt, which means the square root of the total number of features will be considered at each split.\n",
    "\n",
    "`sqrt`: The square root of the total number of features will be considered at each split.\n",
    "\n",
    "`log2`: The base-2 logarithm of the total number of features will be considered at each split.\n",
    "You can also specify an integer, which directly sets the number of features to consider at each split, or a float in the range (0, 1), which represents the fraction of features to consider.\n",
    "Including all three categorical options in the parameter space allows the Bayesian optimization to find the best max_features value for your specific problem. You can also include the float and integer options if you want to explore a broader range of possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_bayes_opt(X, y, skf):\n",
    "    param_space = {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'max_depth': Integer(3, 30),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'min_samples_leaf': Integer(1, 10),\n",
    "        'criterion': Categorical(['gini', 'entropy']),\n",
    "        'max_features': Categorical(['auto', 'sqrt', 'log2'])\n",
    "    }\n",
    "    custom_scorer = make_scorer(combined_scorer, greater_is_better=True)\n",
    "\n",
    "    bayes_opt = BayesSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=42),\n",
    "        search_spaces=param_space,\n",
    "        scoring=custom_scorer,\n",
    "        n_iter=50,\n",
    "        cv=skf,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    bayes_opt.fit(X, y)\n",
    "\n",
    "    print(\"Best hyperparameters:\", bayes_opt.best_params_)\n",
    "    print(\"Best combined score:\", bayes_opt.best_score_)\n",
    "    \n",
    "    return bayes_opt.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train_glove_0v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into features (X) and labels (y)\n",
    "X = train_glove_0v[features]\n",
    "y = train_glove_0v['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7666 +/- 0.0170\n",
      "Mean F1 Score: 0.7034 +/- 0.0204\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(rfc, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding best Hyparameters for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuhao\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters: OrderedDict([('criterion', 'entropy'), ('max_depth', 23), ('max_features', 'sqrt'), ('min_samples_leaf', 4), ('min_samples_split', 7), ('n_estimators', 236)])\n",
      "Best combined score: 0.5542215922936256\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_bayes_opt(X, y, skf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_optimized = RandomForestClassifier(random_state=42, n_estimators=236, max_depth=23, min_samples_split=7, min_samples_leaf=4, criterion='entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7717 +/- 0.0187\n",
      "Mean F1 Score: 0.7156 +/- 0.0221\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(rfc_optimized, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on test data\n",
    "test_pred = rfc_optimized.predict(test_glove_0v[features])\n",
    "\n",
    "# export to csv\n",
    "df = pd.DataFrame({'id': test_glove_0v['id'], 'target': test_pred})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('test_results/predicted_target_glove_0d.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of train_glove_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_glove_custom[features]\n",
    "y = train_glove_custom['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7674 +/- 0.0167\n",
      "Mean F1 Score: 0.7070 +/- 0.0186\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(rfc, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding best Hyparameters for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuhao\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: OrderedDict([('criterion', 'gini'), ('max_depth', 14), ('max_features', 'auto'), ('min_samples_leaf', 10), ('min_samples_split', 4), ('n_estimators', 156)])\n",
      "Best combined score: 0.5527176864410794\n"
     ]
    }
   ],
   "source": [
    "best_rfc = train_and_evaluate_bayes_opt(X, y, skf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_optimized = RandomForestClassifier(random_state=42, n_estimators=156, max_depth=14, min_samples_split=4, min_samples_leaf=10, criterion='gini', max_features='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7724 +/- 0.0197\n",
      "Mean F1 Score: 0.7150 +/- 0.0233\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(rfc_optimized, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on test data\n",
    "test_pred = rfc_optimized.predict(test_glove_custom[features])\n",
    "\n",
    "# export to csv\n",
    "df = pd.DataFrame({'id': test_glove_custom['id'], 'target': test_pred})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('test_results/predicted_target_glove_custom.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of train_word2vec_50d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_word2vec[features]\n",
    "y = train_word2vec['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7679 +/- 0.0163\n",
      "Mean F1 Score: 0.7101 +/- 0.0192\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(rfc, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding best Hyparameters for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters: OrderedDict([('criterion', 'entropy'), ('max_depth', 23), ('max_features', 'sqrt'), ('min_samples_leaf', 4), ('min_samples_split', 7), ('n_estimators', 236)])\n",
      "Best combined score: 0.5539742773505802\n"
     ]
    }
   ],
   "source": [
    "best_rfc = train_and_evaluate_bayes_opt(X, y, skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7724 +/- 0.0149\n",
      "Mean F1 Score: 0.7169 +/- 0.0177\n"
     ]
    }
   ],
   "source": [
    "rfc_optimized = RandomForestClassifier(random_state=42, n_estimators=236, max_depth=23, min_samples_split=7, min_samples_leaf=4, criterion='entropy') \n",
    "train_and_evaluate(rfc_optimized, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on test data\n",
    "test_pred = rfc_optimized.predict(test_word2vec[features])\n",
    "\n",
    "# export to csv\n",
    "df = pd.DataFrame({'id': test_word2vec['id'], 'target': test_pred})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('test_results/predicted_target_word2vec.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of train_fasttext_300d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasttext_features = train_fasttext.drop(['id', 'target'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_fasttext_features\n",
    "y = train_fasttext['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7566 +/- 0.0125\n",
      "Mean F1 Score: 0.6842 +/- 0.0160\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(rfc, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding best Hyparameters for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters: OrderedDict([('criterion', 'entropy'), ('max_depth', 30), ('max_features', 'sqrt'), ('min_samples_leaf', 3), ('min_samples_split', 2), ('n_estimators', 500)])\n",
      "Best combined score: 0.5317004569104965\n"
     ]
    }
   ],
   "source": [
    "best_rfc = train_and_evaluate_bayes_opt(X, y, skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7647 +/- 0.0157\n",
      "Mean F1 Score: 0.6949 +/- 0.0194\n"
     ]
    }
   ],
   "source": [
    "rfc_optimized = RandomForestClassifier(random_state=42, n_estimators=500, max_depth=30, min_samples_split=2, min_samples_leaf=3, criterion='entropy') \n",
    "train_and_evaluate(rfc_optimized, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword_encoded', 'tweet_length', 'punctuation_count',\n",
       "       'embedding_0', 'embedding_1', 'embedding_2', 'embedding_3',\n",
       "       'embedding_4', 'embedding_5',\n",
       "       ...\n",
       "       'embedding_290', 'embedding_291', 'embedding_292', 'embedding_293',\n",
       "       'embedding_294', 'embedding_295', 'embedding_296', 'embedding_297',\n",
       "       'embedding_298', 'embedding_299'],\n",
       "      dtype='object', length=304)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fasttext.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fasttext_no_id = test_fasttext.drop(['id'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on test data\n",
    "test_pred = rfc_optimized.predict(test_fasttext_no_id)\n",
    "\n",
    "# export to csv\n",
    "df = pd.DataFrame({'id': test_fasttext['id'], 'target': test_pred})\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('test_results/predicted_target_fasttext.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classification Test Set Prediction Scores (from Kaggle submission): \n",
    "1. predicted_target_word2vec.csv: `0.75268` \n",
    "2. predicted_target_glove_0d.csv: `0.74532` \n",
    "3. predicted_target_glove_custom.csv: `0.74501` \n",
    "4. predicted_target_fasttext.csv: `0.74164` \n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on prediction scores, we can conclude that `train_word2vec_50d.csv` performed the best in training the Random Forest Classifier with optimized configuration. \n",
    "Interestingly, `train_fasttext_300d.csv`  with 300 columns (6 times more than the glove and wordvec dataset), performed the poorest among all datasets. This means that having a higher embedding dimension may not necessarily mean better accuracy, and may make it worse instead.<br>\n",
    "Even though `train_glove_custom.csv` had the highest validation accuracy, it did not perform the best for the test set. This could mean that the training data might be slightly overfitting the model, resulting in poorer performance to unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
