{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Model with Stratified k-Fold Cross-Validation\n",
    "\n",
    "We will train a Gradient Boosting model using stratified k-fold cross-validation for the 4 different training datasets to determine which datasets is the most suitable for training the final model. We will evaluate the performance of the model using accuracy and F1-score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "The datasets are stored in the `numerical_dataset` folder. We will load them into a list of DataFrames.\n",
    "\n",
    "1. train_data_mod_fasttext_300d_numerical.csv\n",
    "2. train_data_mod_glove_50d_0v_numerical.csv\n",
    "3. train_data_mod_glove_50d_custom_numerical.csv\n",
    "4. train_data_mod_word2vec_50d_numerical.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of train dataset filenames\n",
    "datasets = [\n",
    "    \"../numerical_datasets/train_data_mod_fasttext_300d_numerical.csv\",\n",
    "    \"../numerical_datasets/train_data_mod_glove_50d_0v_numerical.csv\",\n",
    "    \"../numerical_datasets/train_data_mod_glove_50d_custom_numerical.csv\",\n",
    "    \"../numerical_datasets/train_data_mod_word2vec_50d_numerical.csv\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Stratified k-Fold Cross-Validation\n",
    "\n",
    "We'll perform stratified k-fold cross-validation for each dataset. This ensures that each validation set has the same distribution of target values as the entire dataset. We'll use 5 folds in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: numerical_datasets/train_data_mod_fasttext_300d_numerical.csv\n",
      "Average accuracy: 0.7612\n",
      "Average F1-score: 0.7086\n",
      "\n",
      "\n",
      "Processing dataset: numerical_datasets/train_data_mod_glove_50d_0v_numerical.csv\n",
      "Average accuracy: 0.7580\n",
      "Average F1-score: 0.7067\n",
      "\n",
      "\n",
      "Processing dataset: numerical_datasets/train_data_mod_glove_50d_custom_numerical.csv\n",
      "Average accuracy: 0.7579\n",
      "Average F1-score: 0.7048\n",
      "\n",
      "\n",
      "Processing dataset: numerical_datasets/train_data_mod_word2vec_50d_numerical.csv\n",
      "Average accuracy: 0.7571\n",
      "Average F1-score: 0.7048\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified k-fold cross-validation for each dataset\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(dataset)\n",
    "\n",
    "    # Define target variable and feature columns\n",
    "    target = \"target\"\n",
    "    features = df.columns.drop([\"id\", \"target\"])\n",
    "\n",
    "    # Set up stratified k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, val_index in skf.split(df, df[target]):\n",
    "        X_train, X_val = df.loc[train_index, features], df.loc[val_index, features]\n",
    "        y_train, y_val = df.loc[train_index, target], df.loc[val_index, target]\n",
    "\n",
    "        # Train the Gradient Boosting model\n",
    "        model = XGBClassifier(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        # Store evaluation metrics\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Print average evaluation metrics for the dataset\n",
    "    print(f\"Average accuracy: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"Average F1-score: {np.mean(f1_scores):.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We will perform hyperparameter tuning using Random Search with 5-fold cross-validation on each dataset. This will help us find the best hyperparameters for the XGBoost Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing numerical_datasets/train_data_mod_fasttext_300d_numerical.csv\n",
      "Best score: 0.747682378581122\n",
      "Best params: OrderedDict([('colsample_bytree', 0.8212749643012855), ('learning_rate', 0.010428072688169624), ('max_depth', 10), ('n_estimators', 198), ('subsample', 0.5815134565446674)])\n",
      "\n",
      "\n",
      "Average accuracy: 0.7741\n",
      "Average F1 Score: 0.7213\n",
      "\n",
      "\n",
      "Processing numerical_datasets/train_data_mod_glove_50d_0v_numerical.csv\n",
      "Best score: 0.7448216322459841\n",
      "Best params: OrderedDict([('colsample_bytree', 1.0), ('learning_rate', 0.01), ('max_depth', 10), ('n_estimators', 92), ('subsample', 0.6265872392721268)])\n",
      "\n",
      "\n",
      "Average accuracy: 0.7703\n",
      "Average F1 Score: 0.7194\n",
      "\n",
      "\n",
      "Processing numerical_datasets/train_data_mod_glove_50d_custom_numerical.csv\n",
      "Best score: 0.7461805091520255\n",
      "Best params: OrderedDict([('colsample_bytree', 0.864064272685718), ('learning_rate', 0.11111325363758288), ('max_depth', 6), ('n_estimators', 53), ('subsample', 0.7246541244464286)])\n",
      "\n",
      "\n",
      "Average accuracy: 0.7718\n",
      "Average F1 Score: 0.7205\n",
      "\n",
      "\n",
      "Processing numerical_datasets/train_data_mod_word2vec_50d_numerical.csv\n",
      "Best score: 0.7508532679756377\n",
      "Best params: OrderedDict([('colsample_bytree', 0.9747935341542013), ('learning_rate', 0.037168532194711314), ('max_depth', 7), ('n_estimators', 189), ('subsample', 0.500914112838274)])\n",
      "\n",
      "\n",
      "Average accuracy: 0.7756\n",
      "Average F1 Score: 0.7261\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def combined_scorer(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return 0.5 * accuracy + 0.5 * f1\n",
    "\n",
    "# Define the search space for Bayesian Optimization\n",
    "param_space = {\n",
    "    'learning_rate': Real(0.01, 0.2),\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'n_estimators': Integer(50, 201),\n",
    "    'subsample': Real(0.5, 1.0),\n",
    "    'colsample_bytree': Real(0.5, 1.0),\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_clf = XGBClassifier(random_state=42)\n",
    "\n",
    "# Create a StratifiedKFold object for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Use BayesSearchCV for hyperparameter tuning\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    search_spaces=param_space,\n",
    "    scoring=make_scorer(combined_scorer),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    n_iter=50,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "def evaluate_datasets(datasets):\n",
    "    best_models = []\n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing {dataset}\")\n",
    "        data = pd.read_csv(dataset)\n",
    "        \n",
    "        X = data.drop(columns=['id', 'target'])\n",
    "        y = data['target']\n",
    "        \n",
    "        bayes_search.fit(X, y)\n",
    "        \n",
    "        print(f\"Best score: {bayes_search.best_score_}\")\n",
    "        print(f\"Best params: {bayes_search.best_params_}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        clf = XGBClassifier(**bayes_search.best_params_, random_state=42)\n",
    "        scores = cross_validate(\n",
    "            estimator=clf,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            scoring={'accuracy': make_scorer(accuracy_score), 'f1': make_scorer(f1_score)},\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        print(f\"Average accuracy: {np.mean(scores['test_accuracy']):.4f}\")\n",
    "        print(f\"Average F1 Score: {np.mean(scores['test_f1']):.4f}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        best_models.append(clf.fit(X, y))\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "best_models = evaluate_datasets(datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of test dataset filenames\n",
    "test_datasets = [\n",
    "    \"../numerical_datasets/test_data_mod_fasttext_300d_numerical.csv\",\n",
    "    \"../numerical_datasets/test_data_mod_glove_50d_0v_numerical.csv\",\n",
    "    \"../numerical_datasets/test_data_mod_glove_50d_custom_numerical.csv\",\n",
    "    \"../numerical_datasets/test_data_mod_word2vec_50d_numerical.csv\",\n",
    "]\n",
    "\n",
    "# Save the ouputs inthe predictions folder\n",
    "output_filenames = [\n",
    "    \"gb_predictions/test_data_mod_fasttext_300d_predictions.csv\",\n",
    "    \"gb_predictions/test_data_mod_glove_50d_0v_predictions.csv\",\n",
    "    \"gb_predictions/test_data_mod_glove_50d_custom_predictions.csv\",\n",
    "    \"gb_predictions/test_data_mod_word2vec_50d_predictions.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing numerical_datasets/test_data_mod_fasttext_300d_numerical.csv\n",
      "Saved predictions to predictions/test_data_mod_fasttext_300d_predictions.csv\n",
      "\n",
      "\n",
      "Processing numerical_datasets/test_data_mod_glove_50d_0v_numerical.csv\n",
      "Saved predictions to predictions/test_data_mod_glove_50d_0v_predictions.csv\n",
      "\n",
      "\n",
      "Processing numerical_datasets/test_data_mod_glove_50d_custom_numerical.csv\n",
      "Saved predictions to predictions/test_data_mod_glove_50d_custom_predictions.csv\n",
      "\n",
      "\n",
      "Processing numerical_datasets/test_data_mod_word2vec_50d_numerical.csv\n",
      "Saved predictions to predictions/test_data_mod_word2vec_50d_predictions.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_predictions_and_save(models, test_datasets, output_filenames):\n",
    "    for idx, (model, test_dataset, output_filename) in enumerate(zip(models, test_datasets, output_filenames)):\n",
    "        print(f\"Processing {test_dataset}\")\n",
    "        \n",
    "        data = pd.read_csv(test_dataset)\n",
    "        X_test = data.drop(columns=['id'])\n",
    "        ids = data['id']\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        output = pd.DataFrame({'id': ids, 'target': y_pred})\n",
    "        output.to_csv(output_filename, index=False)\n",
    "        print(f\"Saved predictions to {output_filename}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "make_predictions_and_save(best_models, test_datasets, output_filenames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here are the results of the predictions:\n",
    "1. test_data_mod_fasttext_300d_predictions.csv = 0.75574\n",
    "2. test_data_mod_glove_50d_0v_predictions.csv = 0.74348\n",
    "3. test_data_mod_glove_50d_custom_predictions.csv = 0.7441\n",
    "4. test_data_mod_word2vec_50d_predictions.csv = 0.75206"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the results, we can conclude that the dataset with Fasttext 300D embeddings performed the best with an accuracy of 0.75574, followed by the dataset with Word2Vec 50D embeddings (0.75206). The datasets with GloVe 50D embeddings had similar performances, with the custom dataset performing slightly better than the zero-vector dataset (0.7441 vs. 0.74348).\n",
    "\n",
    "In summary, the Fasttext 300D dataset seems to be the most suitable for training the final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
