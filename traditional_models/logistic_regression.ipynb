{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data and Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from hyperopt import hp\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data_fasttext_300d = pd.read_csv(\"../numerical_datasets/train_data_mod_fasttext_300d_numerical.csv\")\n",
    "train_data_glove_50d_0v = pd.read_csv(\"../numerical_datasets/train_data_mod_glove_50d_0v_numerical.csv\")\n",
    "train_data_glove_50d_custom = pd.read_csv(\"../numerical_datasets/train_data_mod_glove_50d_custom_numerical.csv\")\n",
    "train_data_word2vec_50d = pd.read_csv(\"../numerical_datasets/train_data_mod_word2vec_50d_numerical.csv\")\n",
    "\n",
    "test_data_fasttext_300d = pd.read_csv(\"../numerical_datasets/test_data_mod_fasttext_300d_numerical.csv\")\n",
    "test_data_glove_50d_0v = pd.read_csv(\"../numerical_datasets/test_data_mod_glove_50d_0v_numerical.csv\")\n",
    "test_data_glove_50d_custom = pd.read_csv(\"../numerical_datasets/test_data_mod_glove_50d_custom_numerical.csv\")\n",
    "test_data_word2vec_50d = pd.read_csv(\"../numerical_datasets/test_data_mod_word2vec_50d_numerical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [(\"fasttext_300d\", train_data_fasttext_300d, test_data_fasttext_300d),\n",
    "            (\"glove_50d_0v\", train_data_glove_50d_0v, test_data_glove_50d_0v),\n",
    "            (\"glove_50d_custom\", train_data_glove_50d_custom, test_data_glove_50d_custom),\n",
    "            (\"word2vec_50d\", train_data_word2vec_50d, test_data_word2vec_50d)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation: Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a linear model that uses a logistic function to transform the output into a probability value between 0 and 1, which can be interpreted as the likelihood of the positive class. It works by finding the optimal values of the model coefficients that maximize the likelihood of the data given the model, typically using maximum likelihood estimation.\n",
    "\n",
    "For each dataset, we evaluate the logistic regression model using stratified k-fold cross-validation and compute evaluation metrics which include: F1-score, precision, recall, accuracy, and ROC AUC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "seed = 69\n",
    "max_iter = 1000\n",
    "lr_model = LogisticRegression(max_iter=max_iter, random_state=seed)\n",
    "\n",
    "# Define the number of folds for stratified k-fold cross validation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define the evaluation metrics\n",
    "eval_metrics = [accuracy_score, f1_score]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression w/ Default Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for dataset: fasttext_300d\n",
      "Average accuracy_score Score: 0.7571287563535211\n",
      "Average f1_score Score: 0.7067025766996066\n",
      "------------------------\n",
      "Results for dataset: glove_50d_0v\n",
      "Average accuracy_score Score: 0.7588353093132632\n",
      "Average f1_score Score: 0.7091203600469612\n",
      "------------------------\n",
      "Results for dataset: glove_50d_custom\n",
      "Average accuracy_score Score: 0.7590983802457802\n",
      "Average f1_score Score: 0.7094373807659478\n",
      "------------------------\n",
      "Results for dataset: word2vec_50d\n",
      "Average accuracy_score Score: 0.7575226293633407\n",
      "Average f1_score Score: 0.7075464109754979\n",
      "------------------------\n",
      "The dataset with the highest average scores is glove_50d_custom, with the following average scores:\n",
      "Average accuracy_score Score: 0.7590983802457802\n",
      "Average f1_score Score: 0.7094373807659478\n"
     ]
    }
   ],
   "source": [
    "# Use a loop to iterate over each dataset, and for each dataset, train and evaluate the model using stratified k-fold cross-validation\n",
    "average_scores = {name: {metric.__name__: 0 for metric in eval_metrics} for name, _, _ in datasets}\n",
    "best_scores = {metric.__name__: -np.inf for metric in eval_metrics}\n",
    "best_dataset = ''\n",
    "\n",
    "for name, train_data, test_data in datasets: # test_data is redundant\n",
    "    X = train_data.drop([\"id\", \"target\"], axis=1)\n",
    "    y = train_data[\"target\"]\n",
    "\n",
    "    scores = {metric.__name__: [] for metric in eval_metrics}\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        lr_model.fit(X_train, y_train)\n",
    "        y_pred = lr_model.predict(X_test)\n",
    "        \n",
    "        for metric in eval_metrics:\n",
    "            score = metric(y_test, y_pred)\n",
    "            scores[metric.__name__].append(score)\n",
    "    \n",
    "    print(f\"Results for dataset: {name}\")\n",
    "    for metric in eval_metrics:\n",
    "        average_scores[name][metric.__name__] = np.mean(scores[metric.__name__])\n",
    "        print(f\"Average {metric.__name__} Score: {average_scores[name][metric.__name__]}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "for metric in eval_metrics:\n",
    "    best_score = -np.inf\n",
    "    for name, _, _ in datasets:\n",
    "        if average_scores[name][metric.__name__] > best_score:\n",
    "            best_score = average_scores[name][metric.__name__]\n",
    "            best_dataset = name\n",
    "    best_scores[metric.__name__] = best_score\n",
    "\n",
    "print(f\"The dataset with the highest average scores is {best_dataset}, with the following average scores:\")\n",
    "for metric in eval_metrics:\n",
    "    print(f\"Average {metric.__name__} Score: {best_scores[metric.__name__]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default configurations of the logistic regression model, the best performing dataset is 'glove_50d_custom'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Hyperparameters using Bayesian Optimisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimisation is used to tune the hyperparameters for our Logistic Regression model. Unlike Grid Search or Random Search, Bayesian Optimisation can be more efficient as it uses information from previous iterations to guide the search towards promising regions in the hyperparameter space. This can result in better performance with fewer evaluations.\n",
    "\n",
    "The hyperparameters to tune are the regularisation strength (C), the penalty functions (l1 or l2), and the solver method (lbfgs, liblinear, or saga)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation on Only Glove 50d Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for dataset glove_50d_custom: OrderedDict([('C', 98.99856540492476), ('penalty', 'l2'), ('solver', 'liblinear')])\n",
      "Results for dataset: glove_50d_custom\n",
      "Average accuracy_score Score: 0.7650100129162738\n",
      "Average f1_score Score: 0.7170666745676758\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Final dataset\n",
    "name, train_data, test_data = \"glove_50d_custom\", train_data_glove_50d_custom, test_data_glove_50d_custom\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "hyperparameter_grid = {\n",
    "    \"C\": Real(0.001, 100.0, prior='log-uniform'),\n",
    "    \"penalty\": Categorical(['l2']),\n",
    "    \"solver\": Categorical(['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "}\n",
    "\n",
    "# This warning occurs when the objective function has been evaluated at a certain point during the Bayesian optimization process and can be safely ignored.\n",
    "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "\n",
    "X_train = train_data.drop([\"id\", \"target\"], axis=1)\n",
    "y_train = train_data[\"target\"]\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=50000, random_state=seed) # max_iter raised to 50000 in order for model to converge\n",
    "\n",
    "# Use Bayesian Optimization to find the best hyperparameters\n",
    "opt = BayesSearchCV(\n",
    "    estimator=lr_model,\n",
    "    search_spaces=hyperparameter_grid,\n",
    "    cv=skf,\n",
    "    n_iter=50,\n",
    "    scoring=\"f1\",\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    " \n",
    "best_params = opt.best_params_\n",
    "print(f\"Best hyperparameters for dataset {name}: {best_params}\")\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "lr_model = LogisticRegression(**best_params, max_iter=50000, random_state=seed)\n",
    "\n",
    "# Evaluate the model using stratified k-fold cross-validation\n",
    "scores = {metric.__name__: [] for metric in eval_metrics}\n",
    "\n",
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    lr_model.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = lr_model.predict(X_val_fold)\n",
    "    \n",
    "    for metric in eval_metrics:\n",
    "        score = metric(y_val_fold, y_pred)\n",
    "        scores[metric.__name__].append(score)\n",
    "\n",
    "print(f\"Results for dataset: {name}\")\n",
    "for metric in eval_metrics:\n",
    "    average_scores[name][metric.__name__] = np.mean(scores[metric.__name__])\n",
    "    print(f\"Average {metric.__name__} Score: {average_scores[name][metric.__name__]}\")\n",
    "print(\"------------------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average accuracy_score Score: 0.7590983802457802 -> 0.7650100129162738"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation on Every Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for dataset fasttext_300d: OrderedDict([('C', 48.852137128109696), ('penalty', 'l2'), ('solver', 'liblinear')])\n",
      "Results for dataset: fasttext_300d\n",
      "Average accuracy_score Score: 0.7656666117343959\n",
      "Average f1_score Score: 0.7178081032544782\n",
      "------------------------\n",
      "Best hyperparameters for dataset glove_50d_0v: OrderedDict([('C', 78.51393437358462), ('penalty', 'l2'), ('solver', 'lbfgs')])\n",
      "Results for dataset: glove_50d_0v\n",
      "Average accuracy_score Score: 0.7648783480284348\n",
      "Average f1_score Score: 0.7171108912535352\n",
      "------------------------\n",
      "Best hyperparameters for dataset glove_50d_custom: OrderedDict([('C', 98.99856540492476), ('penalty', 'l2'), ('solver', 'liblinear')])\n",
      "Results for dataset: glove_50d_custom\n",
      "Average accuracy_score Score: 0.7650100129162738\n",
      "Average f1_score Score: 0.7170666745676758\n",
      "------------------------\n",
      "Best hyperparameters for dataset word2vec_50d: OrderedDict([('C', 100.0), ('penalty', 'l2'), ('solver', 'newton-cg')])\n",
      "Results for dataset: word2vec_50d\n",
      "Average accuracy_score Score: 0.7650104443215419\n",
      "Average f1_score Score: 0.7170096065080616\n",
      "------------------------\n",
      "The dataset with the highest average scores is fasttext_300d, with the following average scores:\n",
      "Best accuracy_score Score: 0.7656666117343959\n",
      "Best f1_score Score: 0.7178081032544782\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid to search over\n",
    "hyperparameter_grid = {\n",
    "    \"C\": Real(0.001, 100.0, prior='log-uniform'),\n",
    "    \"penalty\": Categorical(['l2']),\n",
    "    \"solver\": Categorical(['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "}\n",
    "\n",
    "# max_iter raised to 50000 in order for model to converge\n",
    "max_iter = 50000\n",
    "\n",
    "average_scores = {name: {metric.__name__: 0 for metric in eval_metrics} for name, _, _ in datasets}\n",
    "best_scores = {metric.__name__: -np.inf for metric in eval_metrics}\n",
    "best_dataset = ''\n",
    "\n",
    "# This warning occurs when the objective function has been evaluated at a certain point during the Bayesian optimization process and can be safely ignored.\n",
    "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "\n",
    "# Loop through each dataset, and for each dataset, train, tune hyperparameters, and evaluate the model using stratified k-fold cross-validation\n",
    "for name, train_data, test_data in datasets: # test_data is redundant\n",
    "    X_train = train_data.drop([\"id\", \"target\"], axis=1)\n",
    "    y_train = train_data[\"target\"]\n",
    "    \n",
    "    lr_model = LogisticRegression(max_iter=max_iter, random_state=seed)\n",
    "    \n",
    "    # Use Bayesian Optimization to find the best hyperparameters\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=lr_model,\n",
    "        search_spaces=hyperparameter_grid,\n",
    "        cv=skf,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    opt.fit(X_train, y_train)\n",
    "        \n",
    "    best_params = opt.best_params_\n",
    "    print(f\"Best hyperparameters for dataset {name}: {best_params}\")\n",
    "    \n",
    "    # Train the model using the best hyperparameters\n",
    "    lr_model = LogisticRegression(**best_params, max_iter=max_iter, random_state=seed)\n",
    "    \n",
    "    # Evaluate the model using stratified k-fold cross-validation\n",
    "    scores = {metric.__name__: [] for metric in eval_metrics}\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        lr_model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = lr_model.predict(X_val_fold)\n",
    "        \n",
    "        for metric in eval_metrics:\n",
    "            score = metric(y_val_fold, y_pred)\n",
    "            scores[metric.__name__].append(score)\n",
    "    \n",
    "    print(f\"Results for dataset: {name}\")\n",
    "    for metric in eval_metrics:\n",
    "        average_scores[name][metric.__name__] = np.mean(scores[metric.__name__])\n",
    "        print(f\"Average {metric.__name__} Score: {average_scores[name][metric.__name__]}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "for metric in eval_metrics:\n",
    "    best_score = -np.inf\n",
    "    for name, _, _ in datasets:\n",
    "        if average_scores[name][metric.__name__] > best_score:\n",
    "            best_score = average_scores[name][metric.__name__]\n",
    "            best_dataset = name\n",
    "    best_scores[metric.__name__] = best_score\n",
    "\n",
    "print(f\"The dataset with the highest average scores is {best_dataset}, with the following average scores:\")\n",
    "for metric in eval_metrics:\n",
    "    print(f\"Best {metric.__name__} Score: {best_scores[metric.__name__]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to further improve the performance of the logistic regression model, we used the Standard Scaler to normalize our feature variables.\n",
    "\n",
    "Scaling the datasets is an important step in logistic regression because it helps to ensure convergence of the optimization process by bringing features to the same scale, reduce the influence of outliers by making their impact comparable to other data points and improve the interpretability of coefficients by making them comparable and interpretable, thus potentially improving the model's performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_scaled = []\n",
    "\n",
    "for name, train_data, test_data in datasets:\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # fit the scaler on the train data\n",
    "    scaler.fit(train_data.drop(columns=['id', 'target']))\n",
    "    \n",
    "    # transform the train data\n",
    "    train_data_scaled = train_data.copy()\n",
    "    train_data_scaled[train_data.columns.difference(['id', 'target'])] = scaler.transform(train_data.drop(columns=['id', 'target']))\n",
    "    \n",
    "    # transform the test data\n",
    "    test_data_scaled = test_data.copy()\n",
    "    test_data_scaled[test_data.columns.difference(['id'])] = scaler.transform(test_data.drop(columns=['id']))\n",
    "    \n",
    "    # append '_scaled' to the name\n",
    "    name_scaled = name + '_scaled'\n",
    "    \n",
    "    # store the scaled train and test data into a new tuple and append it to the list\n",
    "    datasets_scaled.append((name_scaled, train_data_scaled, test_data_scaled))\n",
    "    \n",
    "\n",
    "# for name, train_data, test_data in datasets_scaled:\n",
    "#     print(\"dataset: \"+ name)\n",
    "#     print(train_data.head())\n",
    "#     print(test_data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation on Scaled Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for dataset fasttext_300d_scaled: OrderedDict([('C', 0.002988802667390649), ('penalty', 'l2'), ('solver', 'liblinear')])\n",
      "Results for dataset: fasttext_300d_scaled\n",
      "Average accuracy_score Score: 0.7664553931266787\n",
      "Average f1_score Score: 0.7210398101940598\n",
      "------------------------\n",
      "Best hyperparameters for dataset glove_50d_0v_scaled: OrderedDict([('C', 0.0034381476447487057), ('penalty', 'l2'), ('solver', 'liblinear')])\n",
      "Results for dataset: glove_50d_0v_scaled\n",
      "Average accuracy_score Score: 0.7648781754663275\n",
      "Average f1_score Score: 0.718438160277155\n",
      "------------------------\n",
      "Best hyperparameters for dataset glove_50d_custom_scaled: OrderedDict([('C', 0.0034381476447487057), ('penalty', 'l2'), ('solver', 'liblinear')])\n",
      "Results for dataset: glove_50d_custom_scaled\n",
      "Average accuracy_score Score: 0.7643531552549907\n",
      "Average f1_score Score: 0.7176344861675744\n",
      "------------------------\n",
      "Best hyperparameters for dataset word2vec_50d_scaled: OrderedDict([('C', 13.301525779043082), ('penalty', 'l2'), ('solver', 'saga')])\n",
      "Results for dataset: word2vec_50d_scaled\n",
      "Average accuracy_score Score: 0.7663236419577861\n",
      "Average f1_score Score: 0.7179181830400487\n",
      "------------------------\n",
      "The dataset with the highest average scores is fasttext_300d_scaled, with the following average scores:\n",
      "Best accuracy_score Score: 0.7664553931266787\n",
      "Best f1_score Score: 0.7210398101940598\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid to search over\n",
    "hyperparameter_grid = {\n",
    "    \"C\": Real(0.001, 100.0, prior='log-uniform'),\n",
    "    \"penalty\": Categorical(['l2']),\n",
    "    \"solver\": Categorical(['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "}\n",
    "\n",
    "best_params_dict = {}\n",
    "\n",
    "# max_iter raised to 50000 in order for model to converge\n",
    "max_iter = 50000\n",
    "\n",
    "average_scores = {name: {metric.__name__: 0 for metric in eval_metrics} for name, _, _ in datasets_scaled}\n",
    "best_scores = {metric.__name__: -np.inf for metric in eval_metrics}\n",
    "best_dataset = ''\n",
    "\n",
    "# This warning occurs when the objective function has been evaluated at a certain point during the Bayesian optimization process and can be safely ignored.\n",
    "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "\n",
    "# Loop through each scaled dataset, and for each dataset, train, tune hyperparameters, and evaluate the model using stratified k-fold cross-validation\n",
    "for name, train_data, test_data in datasets_scaled: # test_data is redundant\n",
    "    X_train = train_data.drop([\"id\", \"target\"], axis=1)\n",
    "    y_train = train_data[\"target\"]\n",
    "    \n",
    "    lr_model = LogisticRegression(max_iter=max_iter, random_state=seed)\n",
    "    \n",
    "    # Use Bayesian Optimization to find the best hyperparameters\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=lr_model,\n",
    "        search_spaces=hyperparameter_grid,\n",
    "        cv=skf,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    opt.fit(X_train, y_train)\n",
    "        \n",
    "    best_params = opt.best_params_\n",
    "    print(f\"Best hyperparameters for dataset {name}: {best_params}\")\n",
    "    \n",
    "    # Store the best parameters for this dataset in the dictionary\n",
    "    best_params_dict[name] = best_params\n",
    "    \n",
    "    # Train the model using the best hyperparameters\n",
    "    lr_model = LogisticRegression(**best_params, max_iter=max_iter, random_state=seed)\n",
    "    \n",
    "    # Evaluate the model using stratified k-fold cross-validation\n",
    "    scores = {metric.__name__: [] for metric in eval_metrics}\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        lr_model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = lr_model.predict(X_val_fold)\n",
    "        \n",
    "        for metric in eval_metrics:\n",
    "            score = metric(y_val_fold, y_pred)\n",
    "            scores[metric.__name__].append(score)\n",
    "    \n",
    "    print(f\"Results for dataset: {name}\")\n",
    "    for metric in eval_metrics:\n",
    "        average_scores[name][metric.__name__] = np.mean(scores[metric.__name__])\n",
    "        print(f\"Average {metric.__name__} Score: {average_scores[name][metric.__name__]}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "for metric in eval_metrics:\n",
    "    best_score = -np.inf\n",
    "    for name, _, _ in datasets_scaled:\n",
    "        if average_scores[name][metric.__name__] > best_score:\n",
    "            best_score = average_scores[name][metric.__name__]\n",
    "            best_dataset = name\n",
    "    best_scores[metric.__name__] = best_score\n",
    "\n",
    "print(f\"The dataset with the highest average scores is {best_dataset}, with the following average scores:\")\n",
    "for metric in eval_metrics:\n",
    "    print(f\"Best {metric.__name__} Score: {best_scores[metric.__name__]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fasttext_300d_scaled': OrderedDict([('C', 0.002988802667390649), ('penalty', 'l2'), ('solver', 'liblinear')]), 'glove_50d_0v_scaled': OrderedDict([('C', 0.0034381476447487057), ('penalty', 'l2'), ('solver', 'liblinear')]), 'glove_50d_custom_scaled': OrderedDict([('C', 0.0034381476447487057), ('penalty', 'l2'), ('solver', 'liblinear')]), 'word2vec_50d_scaled': OrderedDict([('C', 13.301525779043082), ('penalty', 'l2'), ('solver', 'saga')])}\n"
     ]
    }
   ],
   "source": [
    "print(best_params_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Optimised Model on Each Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_pred_lr = []\n",
    "\n",
    "for name, train_data, test_data in datasets_scaled:\n",
    "    X_train = train_data.drop([\"id\", \"target\"], axis=1)\n",
    "    y_train = train_data[\"target\"]\n",
    "    \n",
    "    X_test = test_data.drop(\"id\", axis=1)\n",
    "    \n",
    "    lr_model = LogisticRegression(**best_params_dict[best_dataset], max_iter=max_iter, random_state=seed)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the target values of the test set\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    \n",
    "    # Create a copy of the test data and add the predicted values to it\n",
    "    test_data_pred = test_data.copy()\n",
    "    test_data_pred['target'] = y_pred\n",
    "    \n",
    "    # Store test data with predicted values to list\n",
    "    test_data_pred_lr.append(('test_data_pred_lr_'+name, test_data_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export predicted test data to CSV file\n",
    "output_dir = 'logistic_regression_predictions'\n",
    "\n",
    "for name, test_data in test_data_pred_lr:\n",
    "    filename = os.path.join(output_dir, f'{name}.csv')\n",
    "    test_data.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_fasttext_300d_pred = pd.read_csv(\"logistic_regression_predictions/test_data_pred_lr_fasttext_300d_scaled.csv\")\n",
    "test_data_glove_50d_0v_pred = pd.read_csv(\"logistic_regression_predictions/test_data_pred_lr_glove_50d_0v_scaled.csv\")\n",
    "test_data_glove_50d_custom_pred = pd.read_csv(\"logistic_regression_predictions/test_data_pred_lr_glove_50d_custom_scaled.csv\")\n",
    "test_data_word2vec_50d_pred = pd.read_csv(\"logistic_regression_predictions/test_data_pred_lr_word2vec_50d_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_submission = [(\"fasttext_300d\", test_data_fasttext_300d_pred),\n",
    "            (\"glove_50d_0v\", test_data_glove_50d_0v_pred),\n",
    "            (\"glove_50d_custom\", test_data_glove_50d_custom_pred),\n",
    "            (\"word2vec_50d\", test_data_word2vec_50d_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, test_data in datasets_submission:\n",
    "    # submission format\n",
    "    submission = test_data[['id', 'target']]\n",
    "    \n",
    "    # Export the merged submission DataFrame to a CSV file\n",
    "    submission.to_csv(f'logistic_regression_predictions/kaggle_submission/submission_{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n",
      "3263\n",
      "3263\n",
      "3263\n"
     ]
    }
   ],
   "source": [
    "for name, test_data in datasets_submission:\n",
    "    print(test_data.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    " \n",
    "Logistic Regression Test Set Prediction Scores (from Kaggle submission): \n",
    "1. submission_word2vec_50d.csv: 0.7346 \n",
    "2. submission_glove_50d_custom.csv: 0.73398 \n",
    "3. submission_glove_50d_0v.csv: 0.73398 \n",
    "4. submission_fasttext_300d.csv: 0.73858 \n",
    " \n",
    "Overall, the best performing dataset is the one that uses fasttext_300d. This aligns with the accuracy scores from the previous cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
