{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train = pd.read_csv('../train_data_mod.csv')\n",
    "test = pd.read_csv('../test_data_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod = train.copy()\n",
    "test_mod = test.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  instead of using the preprocessed text, we will use the original text applied.\n",
    "- This is because BERT can learn context and relationships between words (including mispelled words), which makes it different from standard preprocessing techniques. \n",
    "- It also has it's own special tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 513kB/s]\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 6.16MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 3.61kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 411/411 [00:00<?, ?B/s] \n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_distil = BertTokenizer.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target', 'preprocess_text',\n",
       "       'bigram', 'trigram', 'pos', 'keyword_encoded', 'tweet_length',\n",
       "       'punctuation_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mod.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "drop_cols = ['keyword', 'location', 'preprocess_text','bigram','trigram', 'pos']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mod.drop(drop_cols, axis=1, inplace=True)\n",
    "test_mod.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'target', 'keyword_encoded', 'tweet_length',\n",
       "       'punctuation_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mod.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'keyword_encoded', 'tweet_length', 'punctuation_count'], dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mod.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['keyword_encoded', 'tweet_length', 'punctuation_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have a dataset X that you want to standardize\n",
    "\n",
    "# Create an instance of the StandardScaler class\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the dataset\n",
    "scaler.fit(train_mod[numerical_features])\n",
    "\n",
    "# Transform the dataset using the fitted scaler\n",
    "train_mod_scaled = scaler.transform(train_mod[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset\n",
    "This is class that will contain the following:\n",
    "- Constructor to take in text and numerical features, corresponding targets, BERT tokenizer, maximum tokens in 1 tweet sentence\n",
    "- length function to return the length of the text\n",
    "- get_items function so that given an input, it will retrieve the corresponding text, numerical feature, target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, numerical_features, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.numerical_features = numerical_features\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        num_feat = self.numerical_features[item]\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'numerical_features': torch.tensor(num_feat, dtype=torch.float),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(text_column, tokenizer):\n",
    "    max_len = 0\n",
    "    for text in text_column:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        # print(tokens)\n",
    "        max_len = max(max_len, len(tokens))\n",
    "    print(\"Max length: \", max_len, \" tokens\")\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  104  tokens\n",
      "Max length:  99  tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length(train_mod['text'], tokenizer)\n",
    "max_length(test_mod['text'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(text_feature, numerical_features, targets, tokenizer, max_len, batch_size):\n",
    "    dataset = CustomDataset(\n",
    "        texts=text_feature.to_numpy(),\n",
    "        numerical_features=numerical_features.to_numpy(),\n",
    "        targets=targets.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithNumericalFeatures(BertForSequenceClassification):\n",
    "    def __init__(self, config, num_numerical_features):\n",
    "        super().__init__(config)\n",
    "        self.num_numerical_features = num_numerical_features\n",
    "        # Concatenates the numerical features to the output of the BERT model\n",
    "        # This is then represented as a Linear Layer which is then passed to the Dropout and Classifier layers\n",
    "        self.dense = torch.nn.Linear(config.hidden_size + num_numerical_features, config.hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        # We have an output layer with 2 nodes\n",
    "        # This will output the logits, and then be transformed through a softmax function to get the probabilities\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, numerical_features, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = torch.cat((pooled_output, numerical_features), dim=1)\n",
    "        pooled_output = self.dense(pooled_output)\n",
    "        pooled_output = torch.nn.ReLU()(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1).float())\n",
    "            else:\n",
    "                loss_fct = torch.nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return ((logits,),) + outputs[2:] if loss is None else ((loss, logits),) + outputs[2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        numerical_features = d[\"numerical_features\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, numerical_features=numerical_features, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    outputs = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            numerical_features = d[\"numerical_features\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, numerical_features=numerical_features, attention_mask=attention_mask)[0]\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            outputs.extend(preds)\n",
    "            targets_list.extend(targets)\n",
    "\n",
    "    outputs = torch.stack(outputs).cpu()\n",
    "    targets_list = torch.stack(targets_list).cpu()\n",
    "\n",
    "    return accuracy_score(targets_list, outputs), f1_score(targets_list, outputs, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, n_splits=5, epochs=5):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Set up cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(df, df.target)):\n",
    "        print(f'FOLD {fold + 1}/{n_splits}')\n",
    "\n",
    "        train_df = df.loc[train_index].reset_index(drop=True)\n",
    "        val_df = df.loc[val_index].reset_index(drop=True)\n",
    "\n",
    "        # Set up DataLoaders\n",
    "        train_data_loader = create_data_loader(train_df, tokenizer, max_len=128, batch_size=16)\n",
    "        val_data_loader = create_data_loader(val_df, tokenizer, max_len=128, batch_size=16)\n",
    "\n",
    "        model = BertWithNumericalFeatures.from_pretrained('bert-base-cased', num_numerical_features=train_df.numerical_features.shape[1], num_labels=2)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        total_steps = len(train_data_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
    "        best_f1 = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "            train_loss = train_epoch(model, train_data_loader, optimizer, device, scheduler)\n",
    "            print(f'Train loss: {train_loss}')\n",
    "\n",
    "            acc, f1 = eval_model(model, val_data_loader, device)\n",
    "            print(f'Val Accuracy: {acc}, F1: {f1}')\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                torch.save(model.state_dict(), f'model_{fold + 1}.bin')\n",
    "\n",
    "        fold_scores.append(best_f1)\n",
    "\n",
    "    print(f'Mean F1 score: {np.mean(fold_scores)}, std: {np.std(fold_scores)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
