{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Model for Disaster Tweet Classification\n",
    "\n",
    "In this notebook, we will train a GPT-2 model to categorize tweets as disaster or not.\n",
    "\n",
    "We will first use the unprocessed `text` feature from train.csv to train the GPT-2 model. This approach allows us to evaluate the transformer's performance in comparison to traditional models when provided with a less optimized feature for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2ForSequenceClassification, TrainingArguments, Trainer, TextClassificationPipeline\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../preprocessing/train.csv')\n",
    "test = pd.read_csv('../preprocessing/test.csv')\n",
    "\n",
    "train_data = train.copy()\n",
    "test_data = test.copy()\n",
    "\n",
    "# Split the training data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data[\"text\"], train_data[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 tokenizer, model, and configuration\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "config = GPT2Config.from_pretrained(\"gpt2\", num_labels=2)\n",
    "config.pad_token_id = tokenizer.eos_token_id\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "# Set a padding token for the GPT-2 tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the training and validation data\n",
    "train_encodings = tokenizer(X_train.to_list(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val.to_list(), truncation=True, padding=True)\n",
    "\n",
    "# Create a dataset object for the trainer\n",
    "class DisasterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = DisasterDataset(train_encodings, y_train.to_list())\n",
    "val_dataset = DisasterDataset(val_encodings, y_val.to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "test_pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "predictions = test_pipeline(test_data[\"text\"].to_list())\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "test_data[\"target\"] = [prediction[\"label\"].split(\"_\")[-1] for prediction in predictions]\n",
    "test_data[[\"id\", \"target\"]].to_csv(\"predictions/gpt2_predictions.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impressive Results\n",
    "\n",
    "It achieved an accuracy score of 0.81918 which is significantly higher than the scores achieved by any other models we have trained, and this is before any preprocessing of the dataset. Since it achieved much better results than the previous models we have trained, we decided to continue fine-tuning it.\n",
    "\n",
    "We decided to use the `text`, `keyword`, `tweet_count` and `punctuation_count` features to train the GPT-2 model. These features have been proven to display strong relationship with `target` in data_preprocessing.ipynb.\n",
    "\n",
    "Since the GPT-2 model only takes in a single text input, our plan is to concatenate all of the features into one `combined_text` feature and pass it to GPT-2 model as a text input to train it.\n",
    "\n",
    "Examples of how combined_text will look like:\n",
    "\n",
    "1. \"Courageous and honest analysis of need to use Atomic Bomb in 1945. #Hiroshima70 Japanese military refused surrender. https://t.co/VhmtyTptGR (Keyword: military , Tweet Length: 140, Punctuation Count: 8)\"\n",
    "2. \"Typhoon Soudelor kills 28 in China and Taiwan (Keyword: , Tweet length: 45, Punctuation Count: 0)\"\n",
    "\n",
    "By concatenating the `keyword`, `tweet_length` and `punctuation_length` at the end of `text` explicitly in parentheses, it can help the GPT-2 model better understand the data with more context, and potentially perform better at predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(data, text):\n",
    "    data[text] = data['text'] + ' (Keyword: ' + data['keyword'].fillna('') + ',' + ' Tweet length: ' + data['tweet_length'].astype(str) + ',' + ' Punctuation Count: ' + data['punctuation_count'].astype(str) +')'\n",
    "    return data\n",
    "\n",
    "# Load the train dataset with tweet_count and punctuation_count\n",
    "train_meta_data = pd.read_csv('../preprocessing/train_data_mod.csv')\n",
    "train_meta_data = train_meta_data.drop(['target', 'text', 'keyword'], axis=1)\n",
    "\n",
    "# Load the test dataset with tweet_count and punctuation_count\n",
    "test_meta_data = pd.read_csv('../preprocessing/test_data_mod.csv')\n",
    "test_meta_data = test_meta_data.drop(['text', 'keyword'], axis=1)\n",
    "\n",
    "# Merge the data\n",
    "train_data = train_data.merge(train_meta_data, on='id')\n",
    "test_data = test_data.merge(test_meta_data, on='id')\n",
    "\n",
    "# Concatenate the features\n",
    "train_data = combine_text(train_data, 'combined_text')\n",
    "test_data = combine_text(test_data, 'combined_text')\n",
    "\n",
    "# Split the training data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data[\"combined_text\"], train_data[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the concatenated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "4996        Courageous and honest analysis of need to use Atomic Bomb in 1945. #Hiroshima70 Japanese military refused surrender. https://t.co/VhmtyTptGR (Keyword: military, Tweet length: 140, Punctuation Count: 8)\n",
      "3263                                                   @ZachZaidman @670TheScore wld b a shame if that golf cart became engulfed in flames. #boycottBears (Keyword: engulfed, Tweet length: 98, Punctuation Count: 4)\n",
      "4907    Tell @BarackObama to rescind medals of 'honor' given to US soldiers at the Massacre of Wounded Knee. SIGN NOW &amp; RT! https://t.co/u4r8dRiuAc (Keyword: massacre, Tweet length: 143, Punctuation Count: 12)\n",
      "2855                               Worried about how the CA drought might affect you? Extreme Weather: Does it Dampen Our Economy? http://t.co/fDzzuMyW8i (Keyword: drought, Tweet length: 118, Punctuation Count: 8)\n",
      "4716                                                                       @YoungHeroesID Lava Blast &amp; Power Red #PantherAttack @JamilAzzaini @alifaditha (Keyword: lava, Tweet length: 82, Punctuation Count: 6)\n",
      "Name: combined_text, dtype: object\n",
      "\n",
      "X_val:\n",
      "2644                                                            So you have a new weapon that can cause un-imaginable destruction. (Keyword: destruction, Tweet length: 66, Punctuation Count: 2)\n",
      "2227          The f$&amp;@ing things I do for #GISHWHES Just got soaked in a deluge going for pads and tampons. Thx @mishacollins @/@ (Keyword: deluge, Tweet length: 119, Punctuation Count: 10)\n",
      "5448    DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe CoL police can catch a pickpocket in Liverpool Stree... http://t.co/vXIn1gOq4Q (Keyword: police, Tweet length: 125, Punctuation Count: 12)\n",
      "132             Aftershock back to school kick off was great. I want to thank everyone for making it possible. What a great night. (Keyword: aftershock, Tweet length: 114, Punctuation Count: 3)\n",
      "6845                         in response to trauma Children of Addicts develop a defensive self - one that decreases vulnerability. (3 (Keyword: trauma, Tweet length: 105, Punctuation Count: 3)\n",
      "Name: combined_text, dtype: object\n",
      "\n",
      "test_data:\n",
      "0                                                                  Just happened a terrible car crash (Keyword: , Tweet length: 34, Punctuation Count: 0)\n",
      "1                                    Heard about #earthquake is different cities, stay safe everyone. (Keyword: , Tweet length: 64, Punctuation Count: 3)\n",
      "2    there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all (Keyword: , Tweet length: 96, Punctuation Count: 2)\n",
      "3                                                            Apocalypse lighting. #Spokane #wildfires (Keyword: , Tweet length: 40, Punctuation Count: 3)\n",
      "4                                                       Typhoon Soudelor kills 28 in China and Taiwan (Keyword: , Tweet length: 45, Punctuation Count: 0)\n",
      "Name: combined_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "print(\"X_train:\")\n",
    "print(X_train.head())\n",
    "\n",
    "print(\"\\nX_val:\")\n",
    "print(X_val.head())\n",
    "\n",
    "print(\"\\ntest_data:\")\n",
    "print(test_data[\"combined_text\"].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare GPT-2 Model with the concatenated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "train_encodings = tokenizer(X_train.to_list(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val.to_list(), truncation=True, padding=True)\n",
    "\n",
    "train_dataset = DisasterDataset(train_encodings, y_train.to_list())\n",
    "val_dataset = DisasterDataset(val_encodings, y_val.to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GPT-2 Model with the concatenated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "predictions = test_pipeline(test_data[\"combined_text\"].to_list())\n",
    "\n",
    "test_data[\"target\"] = [prediction[\"label\"].split(\"_\")[-1] for prediction in predictions]\n",
    "test_data[[\"id\", \"target\"]].to_csv(\"predictions/gpt2_predictions2.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slightly Better Results\n",
    "\n",
    "It achieved an accuracy score of 0.82715 which is slightly higher than the scores achieved by using a non-preprocessed data (0.81918). The increase is lesser than we have expected because we thought that incorporating additional features would have a more significant impact on the model's performance. However, it is still an improvement, and we can consider further refining the model.\n",
    "\n",
    "Here, we decided to clean up the data by\n",
    "\n",
    "1. removing any URLs present in the text (http, https, or www),\n",
    "2. removing any mentions (@) or hashtags (#) from the text,\n",
    "3. removing any non-alphanumeric characters (keeps only letters and numbers) and replacing them with a single space,\n",
    "4. replacing multiple whitespace characters (spaces, tabs, etc.) with a single space and removes any leading/trailing spaces,\n",
    "\n",
    "and train the GPT-2 model with the cleaned data. This reduces noise in the dataset and keeps the data more consistent. We have also decided to not convert characters to lowercase, eliminate stop words, and avoid lemmatization, because the GPT-2 model has the ability to process these words with context, ultimately producing predictions that are more meaningful.\n",
    "\n",
    "Our prediction is that this model will perform slightly better than the previous model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Clean the text column in train and test dataset\n",
    "train_data['text'] = train_data['text'].apply(lambda x: clean_text(x))\n",
    "test_data[\"text\"] = test_data[\"text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Concatenate the cleaned text with other features\n",
    "train_data = combine_text(train_data, 'cleaned_combined_text')\n",
    "test_data = combine_text(test_data, 'cleaned_combined_text')\n",
    "\n",
    "# Split the training data into train set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data[\"cleaned_combined_text\"], train_data[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned X_train:\n",
      "4996    Courageous and honest analysis of need to use Atomic Bomb in 1945 Hiroshima70 Japanese military refused surrender (Keyword: military, Tweet length: 140, Punctuation Count: 8)\n",
      "3263                                                wld b a shame if that golf cart became engulfed in flames boycottBears (Keyword: engulfed, Tweet length: 98, Punctuation Count: 4)\n",
      "4907                Tell to rescind medals of honor given to US soldiers at the Massacre of Wounded Knee SIGN NOW amp RT (Keyword: massacre, Tweet length: 143, Punctuation Count: 12)\n",
      "2855                          Worried about how the CA drought might affect you Extreme Weather Does it Dampen Our Economy (Keyword: drought, Tweet length: 118, Punctuation Count: 8)\n",
      "4716                                                                                    Lava Blast amp Power Red PantherAttack (Keyword: lava, Tweet length: 82, Punctuation Count: 6)\n",
      "Name: cleaned_combined_text, dtype: object\n",
      "\n",
      "Cleaned X_val:\n",
      "2644                                                  So you have a new weapon that can cause un imaginable destruction (Keyword: destruction, Tweet length: 66, Punctuation Count: 2)\n",
      "2227                         The f amp things I do for GISHWHES Just got soaked in a deluge going for pads and tampons Thx (Keyword: deluge, Tweet length: 119, Punctuation Count: 10)\n",
      "5448                                                        DT RT The CoL police can catch a pickpocket in Liverpool Stree (Keyword: police, Tweet length: 125, Punctuation Count: 12)\n",
      "132     Aftershock back to school kick off was great I want to thank everyone for making it possible What a great night (Keyword: aftershock, Tweet length: 114, Punctuation Count: 3)\n",
      "6845                  in response to trauma Children of Addicts develop a defensive self one that decreases vulnerability 3 (Keyword: trauma, Tweet length: 105, Punctuation Count: 3)\n",
      "Name: cleaned_combined_text, dtype: object\n",
      "\n",
      "Cleaned test_data:\n",
      "0                                                                Just happened a terrible car crash (Keyword: , Tweet length: 34, Punctuation Count: 0)\n",
      "1                                     Heard about earthquake is different cities stay safe everyone (Keyword: , Tweet length: 64, Punctuation Count: 3)\n",
      "2    there is a forest fire at spot pond geese are fleeing across the street I cannot save them all (Keyword: , Tweet length: 96, Punctuation Count: 2)\n",
      "3                                                             Apocalypse lighting Spokane wildfires (Keyword: , Tweet length: 40, Punctuation Count: 3)\n",
      "4                                                     Typhoon Soudelor kills 28 in China and Taiwan (Keyword: , Tweet length: 45, Punctuation Count: 0)\n",
      "Name: cleaned_combined_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned X_train:\")\n",
    "print(X_train.head())\n",
    "\n",
    "print(\"\\nCleaned X_val:\")\n",
    "print(X_val.head())\n",
    "\n",
    "print(\"\\nCleaned test_data:\")\n",
    "print(test_data[\"cleaned_combined_text\"].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare GPT-2 Model with the cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "train_encodings = tokenizer(X_train.to_list(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val.to_list(), truncation=True, padding=True)\n",
    "\n",
    "train_dataset = DisasterDataset(train_encodings, y_train.to_list())\n",
    "val_dataset = DisasterDataset(val_encodings, y_val.to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GPT-2 Model with the cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "predictions = test_pipeline(test_data[\"cleaned_combined_text\"].to_list())\n",
    "\n",
    "test_data[\"target\"] = [prediction[\"label\"].split(\"_\")[-1] for prediction in predictions]\n",
    "test_data[[\"id\", \"target\"]].to_csv(\"predictions/gpt2_predictions3.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Result\n",
    "\n",
    "Indeed, the accuracy score is very slightly better at 0.8296 (vs 0.82715) with the cleaned data.\n",
    "\n",
    "We will further fine-tune the GPT-2 model by optimizing `Learning Rate` and `Number of Epochs`. We decided to perform Grid Search to find the best combination of learning rate and number of epochs.\n",
    "\n",
    "This involves training and evaluating the model with different combinations of learning rates and the number of epochs, and the best model with the highest validation accuracy will be used as the final model for predictions. This approach can help in finding the optimal hyperparameters for the model, leading to improved performance and potentially better prediction results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Learning Rate and Number of Epochs\n",
    "\n",
    "`learning_rates = [1e-5, 5e-5, 1e-4]`: These learning rates are chosen because they are within the typical range used in practice for fine-tuning pre-trained models like GPT-2. A smaller learning rate, like 1e-5, might lead to slower convergence but can be more stable, while a larger learning rate, like 1e-4, can speed up the training process but may cause overshooting and instability in training. The value 5e-5 is chosen as a middle ground between these two extremes.\n",
    "\n",
    "`num_epochs = [3, 5, 7]`: These values for the number of epochs are chosen based on the assumption that the pre-trained GPT-2 model has already learned useful text representations, and a smaller number of epochs might be sufficient for fine-tuning. Using a smaller number of epochs can help prevent overfitting and reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_accuracy(model, val_dataset):\n",
    "    val_pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "    # Convert tokenized inputs back to original text\n",
    "    val_texts = [tokenizer.decode(x[\"input_ids\"], skip_special_tokens=True) for x in val_dataset]\n",
    "    val_predictions = val_pipeline(val_texts)\n",
    "    val_labels = [x[\"labels\"].tolist() for x in val_dataset]\n",
    "    val_accuracy = accuracy_score(val_labels, [int(prediction[\"label\"].split(\"_\")[-1]) for prediction in val_predictions])\n",
    "    return val_accuracy\n",
    "\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "num_epochs = [3, 5, 7]\n",
    "\n",
    "best_model = None\n",
    "best_val_accuracy = 0\n",
    "best_lr = None\n",
    "best_epoch = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epoch in num_epochs:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./gpt2_results_lr{lr}_epoch{epoch}\",\n",
    "            num_train_epochs=epoch,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=100,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"no\",\n",
    "            seed=42,\n",
    "            learning_rate=lr,\n",
    "        )\n",
    "        \n",
    "        model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", config=config)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        val_accuracy = compute_val_accuracy(model, val_dataset)\n",
    "        print(f\"Learning rate: {lr}, Num Epochs: {epoch}, Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = model\n",
    "            best_lr = lr\n",
    "            best_epoch = epoch\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_accuracy}, Best learning rate: {best_lr}, Best number of epochs: {best_epoch}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = TextClassificationPipeline(model=best_model, tokenizer=tokenizer)\n",
    "predictions = test_pipeline(test_data[\"cleaned_combined_text\"].to_list())\n",
    "\n",
    "test_data[\"target\"] = [prediction[\"label\"].split(\"_\")[-1] for prediction in predictions]\n",
    "test_data[[\"id\", \"target\"]].to_csv(\"predictions/gpt2_best_predictions.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unexpected Result\n",
    "\n",
    "It is unexpected that the best model with the optimal combination of learning rate and number of epochs **performed the worst**, with an accuracy score of 0.81366. Here are some possible reasons we can think of for this:\n",
    "\n",
    "1. **Overfitting**: The model might have overfit the training data due to the chosen combination of learning rate and number of epochs. This can happen when the model becomes too specialized in learning the patterns in the training data, and as a result, performs poorly on new, unseen data.\n",
    "\n",
    "2. **Local minima**: It is possible that the model got stuck in a local minimum during the training process, which may have resulted in suboptimal performance. The chosen learning rate and number of epochs might not have been suitable to escape the local minimum and reach a better solution.\n",
    "\n",
    "3. **Randomness**: The training process involves a certain amount of randomness, which can lead to different results with different runs, even with the same hyperparameters. The worse performance might be attributed to the inherent randomness in the training process, and perhaps with additional runs, the performance might improve."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, while the GPT-2 model initially showed promising results, the fine-tuning attempts with various preprocessing techniques and hyperparameter combinations did not yield the expected improvements. This result shocked us as we initially believed that preprocessing the data and fine-tuning the hyperparameters would lead to significant improvements in the model's performance.\n",
    "\n",
    "Nevertheless, the GPT-2 model still performed the best out of all the other models we have trained. It was an unexpected outcome but an eye-opening experience that has allowed us to draw several valuable insights:\n",
    "\n",
    "1. **Importance of raw data**: The fact that the GPT-2 model performed really well on raw data suggests that the context and structure of the original data might be more important than initially thought. This highlights the importance of carefully considering the impact of preprocessing on the model's performance.\n",
    "\n",
    "2. **Complexity of fine-tuning**: Fine-tuning a pre-trained model like GPT-2 can be a complex process, and finding the optimal combination of hyperparameters might not always lead to the best results. This underlines the importance of exploring different strategies and being open to experimentation during the fine-tuning process.\n",
    "\n",
    "3. **Model robustness**: The GPT-2 model's ability to perform well on raw data, without any preprocessing or cleaning, showcases the robustness of this model. It demonstrates that it can effectively handle noisy and unstructured data, making it a strong contender for natural language processing tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
