{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data and Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # numpy=1.23.5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data_fasttext_300d = pd.read_csv(\"numerical_datasets/train_data_mod_fasttext_300d_numerical.csv\")\n",
    "train_data_glove_50d_0v = pd.read_csv(\"numerical_datasets/train_data_mod_glove_50d_0v_numerical.csv\")\n",
    "train_data_glove_50d_custom = pd.read_csv(\"numerical_datasets/train_data_mod_glove_50d_custom_numerical.csv\")\n",
    "train_data_word2vec_50d = pd.read_csv(\"numerical_datasets/train_data_mod_word2vec_50d_numerical.csv\")\n",
    "\n",
    "test_data_fasttext_300d = pd.read_csv(\"numerical_datasets/test_data_mod_fasttext_300d_numerical.csv\")\n",
    "test_data_glove_50d_0v = pd.read_csv(\"numerical_datasets/test_data_mod_glove_50d_0v_numerical.csv\")\n",
    "test_data_glove_50d_custom = pd.read_csv(\"numerical_datasets/test_data_mod_glove_50d_custom_numerical.csv\")\n",
    "test_data_word2vec_50d = pd.read_csv(\"numerical_datasets/test_data_mod_word2vec_50d_numerical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [(\"fasttext_300d\", train_data_fasttext_300d, test_data_fasttext_300d),\n",
    "            (\"glove_50d_0v\", train_data_glove_50d_0v, test_data_glove_50d_0v),\n",
    "            (\"glove_50d_custom\", train_data_glove_50d_custom, test_data_glove_50d_custom),\n",
    "            (\"word2vec_50d\", train_data_word2vec_50d, test_data_word2vec_50d)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation: Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a linear model that uses a logistic function to transform the output into a probability value between 0 and 1, which can be interpreted as the likelihood of the positive class. It works by finding the optimal values of the model coefficients that maximize the likelihood of the data given the model, typically using maximum likelihood estimation.\n",
    "\n",
    "For each dataset, we evaluate the logistic regression model using stratified k-fold cross-validation and compute evaluation metrics which include: F1-score, precision, recall, accuracy, and ROC AUC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale datasets\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# datasets_scaled = []\n",
    "\n",
    "# for name, train_data, test_data in datasets:\n",
    "#     X_train = train_data.drop(\"target\", axis=1)\n",
    "#     y_train = train_data[\"target\"]\n",
    "#     X_test = test_data\n",
    "    \n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "#     y_train_scaled = y_train\n",
    "    \n",
    "#     train_data_scaled = pd.concat([pd.DataFrame(X_train_scaled), y_train_scaled.reset_index(drop=True)], axis=1)\n",
    "#     test_data_scaled = pd.DataFrame(X_test_scaled)\n",
    "    \n",
    "#     datasets_scaled.append((name + \"_scaled\", train_data_scaled, test_data_scaled))\n",
    "    \n",
    "# for name, train_data, test_data in datasets_scaled:\n",
    "#     print(train_data.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "seed = 69\n",
    "max_iter = 1000\n",
    "lr_model = LogisticRegression(max_iter=max_iter, random_state=seed)\n",
    "\n",
    "# Define the number of folds for stratified k-fold cross validation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "# Define the evaluation metrics\n",
    "eval_metrics = [accuracy_score, f1_score, precision_score, recall_score, roc_auc_score]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression w/ Default Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for dataset: fasttext_300d\n",
      "Average accuracy_score Score: 0.7571287563535211\n",
      "Average f1_score Score: 0.7067025766996066\n",
      "Average precision_score Score: 0.7343286833841852\n",
      "Average recall_score Score: 0.6811350001167215\n",
      "Average roc_auc_score Score: 0.7477546371352487\n",
      "------------------------\n",
      "Results for dataset: glove_50d_0v\n",
      "Average accuracy_score Score: 0.7588353093132632\n",
      "Average f1_score Score: 0.7091203600469612\n",
      "Average precision_score Score: 0.7360393621803702\n",
      "Average recall_score Score: 0.6841921703200505\n",
      "Average roc_auc_score Score: 0.7496285792074233\n",
      "------------------------\n",
      "Results for dataset: glove_50d_custom\n",
      "Average accuracy_score Score: 0.7590983802457802\n",
      "Average f1_score Score: 0.7094373807659478\n",
      "Average precision_score Score: 0.7363752451976321\n",
      "Average recall_score Score: 0.6844984476037071\n",
      "Average roc_auc_score Score: 0.749896660073218\n",
      "------------------------\n",
      "Results for dataset: word2vec_50d\n",
      "Average accuracy_score Score: 0.7575226293633407\n",
      "Average f1_score Score: 0.7075464109754979\n",
      "Average precision_score Score: 0.7343461554150499\n",
      "Average recall_score Score: 0.6826645189905923\n",
      "Average roc_auc_score Score: 0.7482892469749459\n",
      "------------------------\n",
      "The dataset with the highest average scores is glove_50d_custom, with the following average scores:\n",
      "Best accuracy_score Score: 0.7590983802457802\n",
      "Best f1_score Score: 0.7094373807659478\n",
      "Best precision_score Score: 0.7363752451976321\n",
      "Best recall_score Score: 0.6844984476037071\n",
      "Best roc_auc_score Score: 0.749896660073218\n"
     ]
    }
   ],
   "source": [
    "# Use a loop to iterate over each dataset, and for each dataset, train and evaluate the model using stratified k-fold cross-validation\n",
    "average_scores = {name: {metric.__name__: 0 for metric in eval_metrics} for name, _, _ in datasets}\n",
    "best_scores = {metric.__name__: -np.inf for metric in eval_metrics}\n",
    "best_dataset = ''\n",
    "\n",
    "for name, train_data, test_data in datasets: # test_data is redundant\n",
    "    X = train_data.drop([\"id\", \"target\"], axis=1)\n",
    "    y = train_data[\"target\"]\n",
    "\n",
    "    scores = {metric.__name__: [] for metric in eval_metrics}\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        lr_model.fit(X_train, y_train)\n",
    "        y_pred = lr_model.predict(X_test)\n",
    "        \n",
    "        for metric in eval_metrics:\n",
    "            score = metric(y_test, y_pred)\n",
    "            scores[metric.__name__].append(score)\n",
    "    \n",
    "    print(f\"Results for dataset: {name}\")\n",
    "    for metric in eval_metrics:\n",
    "        average_scores[name][metric.__name__] = np.mean(scores[metric.__name__])\n",
    "        print(f\"Average {metric.__name__} Score: {average_scores[name][metric.__name__]}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "for metric in eval_metrics:\n",
    "    best_score = -np.inf\n",
    "    for name, _, _ in datasets:\n",
    "        if average_scores[name][metric.__name__] > best_score:\n",
    "            best_score = average_scores[name][metric.__name__]\n",
    "            best_dataset = name\n",
    "    best_scores[metric.__name__] = best_score\n",
    "\n",
    "print(f\"The dataset with the highest average scores is {best_dataset}, with the following average scores:\")\n",
    "for metric in eval_metrics:\n",
    "    print(f\"Best {metric.__name__} Score: {best_scores[metric.__name__]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default configurations of the logistic regression model, the best performing dataset is 'glove_50d_custom'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Hyperparameters using Bayesian Optimisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimisation is used to tune the hyperparameters for our Logistic Regression model. Unlike Grid Search or Random Search, Bayesian Optimisation can be more efficient as it uses information from previous iterations to guide the search towards promising regions in the hyperparameter space. This can result in better performance with fewer evaluations.\n",
    "\n",
    "The hyperparameters to tune are the regularisation strength (C), the penalty functions (l1 or l2), and the solver method (lbfgs, liblinear, or saga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid to search over\n",
    "hyperparameter_grid = {\n",
    "    \"C\": Real(0.001, 100.0, prior='log-uniform'),\n",
    "    \"penalty\": Categorical(['l1', 'l2']),\n",
    "    \"solver\": Categorical(['lbfgs', 'liblinear', 'saga']),\n",
    "}\n",
    "\n",
    "average_scores = {name: {metric.__name__: 0 for metric in eval_metrics} for name, _, _ in datasets}\n",
    "best_scores = {metric.__name__: -np.inf for metric in eval_metrics}\n",
    "best_dataset = ''\n",
    "\n",
    "# This warning occurs when the objective function has been evaluated at a certain point during the Bayesian optimization process and can be safely ignored.\n",
    "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "\n",
    "# Loop through each dataset, and for each dataset, train, tune hyperparameters, and evaluate the model using stratified k-fold cross-validation\n",
    "for name, train_data, test_data in datasets: # test_data is redundant\n",
    "    X_train = train_data.drop([\"id\", \"target\"], axis=1)\n",
    "    y_train = train_data[\"target\"]\n",
    "    \n",
    "    lr_model = LogisticRegression(max_iter=max_iter, random_state=seed)\n",
    "    \n",
    "    # Use Bayesian Optimization to find the best hyperparameters\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=lr_model,\n",
    "        search_spaces=hyperparameter_grid,\n",
    "        cv=skf,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        opt.fit(X_train, y_train)\n",
    "    except ValueError as e:\n",
    "        if \"Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty\" in str(e):\n",
    "            print(\"Ignoring lbfgs solver with l1 penalty.\")\n",
    "        else:\n",
    "            raise e\n",
    "        \n",
    "    best_params = opt.best_params_\n",
    "    print(f\"Best hyperparameters for dataset {name}: {best_params}\")\n",
    "    \n",
    "    # Train the model using the best hyperparameters\n",
    "    lr_model = LogisticRegression(**best_params, max_iter=max_iter, random_state=seed)\n",
    "    \n",
    "    # Evaluate the model using stratified k-fold cross-validation\n",
    "    scores = {metric.__name__: [] for metric in eval_metrics}\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        lr_model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = lr_model.predict(X_val_fold)\n",
    "        \n",
    "        for metric in eval_metrics:\n",
    "            score = metric(y_val_fold, y_pred)\n",
    "            scores[metric.__name__].append(score)\n",
    "    \n",
    "    print(f\"Results for dataset: {name}\")\n",
    "    for metric in eval_metrics:\n",
    "        average_scores[name][metric.__name__] = np.mean(scores[metric.__name__])\n",
    "        print(f\"Average {metric.__name__} Score: {average_scores[name][metric.__name__]}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "for metric in eval_metrics:\n",
    "    best_score = -np.inf\n",
    "    for name, _, _ in datasets:\n",
    "        if average_scores[name][metric.__name__] > best_score:\n",
    "            best_score = average_scores[name][metric.__name__]\n",
    "            best_dataset = name\n",
    "    best_scores[metric.__name__] = best_score\n",
    "\n",
    "print(f\"The dataset with the highest average scores is {best_dataset}, with the following average scores:\")\n",
    "for metric in eval_metrics:\n",
    "    print(f\"Best {metric.__name__} Score: {best_scores[metric.__name__]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
