{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding using Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Word2Vec to create word embeddings for our preprocessed text. \n",
    "\n",
    "The advantage of using Word Embedding over other methods like Bag of Words or TF-IDF is that it can capture the semantic meaning of words in the text. This means that words with similar meanings will have similar vector representations. This can help improve the performance of our machine learning model.\n",
    "\n",
    "First, let's import the necessary libraries and train our Word2Vec model on the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_data_mod = pd.read_csv('check.csv')\n",
    "\n",
    "preprocessed_text = train_data_mod['preprocess_text'].tolist()\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=preprocessed_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "word2vec_model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our Word2Vec model, let's use it to obtain word vectors for our preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vector(tokens, word2vec_model, vector_size):\n",
    "   word_vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv.index_to_key]\n",
    "   \n",
    "   if not word_vectors:\n",
    "       return np.zeros(vector_size)\n",
    "   \n",
    "   return np.mean(word_vectors, axis=0)\n",
    "\n",
    "vector_size = 100\n",
    "train_data_mod['word2vec_vectors'] = train_data_mod['preprocess_text'].apply(lambda x: average_word_vector(x, word2vec_model, vector_size))\n",
    "\n",
    "train_data_mod['word2vec_vectors'].head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
